{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNdwFnTO5l2o",
        "colab_type": "code",
        "outputId": "80cd579a-880f-4e33-ba8e-039987d530bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf \n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYNQZnKsVAEg",
        "colab_type": "code",
        "outputId": "1dfcc29d-14af-4ce1-f2e8-4080bf9bed14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount drive\n",
        "drive.mount('/content/gdrive')\n",
        "PROJECT_PATH = '/content/gdrive/My Drive/DL_04/'\n",
        "\n",
        "# List loaded files \n",
        "print(os.listdir(PROJECT_PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "['GoogleNews-vectors-negative300.bin.gz', 'glove.6B.50d.txt', 'glove.6B.100d.txt', 'glove.6B.200d.txt', 'glove.6B.300d.txt', 'glove.twitter.27B.50d.txt', 'glove.twitter.27B.100d.txt', 'glove.twitter.27B.200d.txt', 'test_emoji.csv', 'train_emoji.csv', 'GoogleNews-vectors-negative300.bin']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKQSRU4Tbipb",
        "colab_type": "code",
        "outputId": "f620058a-f420-484a-bece-26dc625ba7b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load train and test dataset \n",
        "train_ds = pd.read_csv(PROJECT_PATH + 'train_emoji.csv',  header=None, usecols=[0,1], names=['sentence', 'emoji'])\n",
        "test_ds = pd.read_csv(PROJECT_PATH + 'test_emoji.csv',  header=None, usecols=[0,1], names=['sentence', 'emoji'])\n",
        "\n",
        "# Show data type - I had to bit modify the test data - they werent in the same format \n",
        "# and in with the tabulator at the end of the sentence\n",
        "display(train_ds.head(2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>emoji</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>never talk to me again</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I am proud of your achievements</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          sentence  emoji\n",
              "0           never talk to me again      3\n",
              "1  I am proud of your achievements      2"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akY7tzdgpZH_",
        "colab_type": "code",
        "outputId": "ec67b05c-42fb-41ef-98ba-2c7433e7af6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# Split train, test dataset\n",
        "X_train, y_train = train_ds['sentence'], train_ds['emoji']\n",
        "X_test, y_test = test_ds['sentence'], test_ds['emoji']\n",
        "\n",
        "\n",
        "display(X_train.head(2))\n",
        "display(y_train.head(2))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0             never talk to me again\n",
              "1    I am proud of your achievements\n",
              "Name: sentence, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0    3\n",
              "1    2\n",
              "Name: emoji, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncWvtyXf1epC",
        "colab_type": "code",
        "outputId": "61dddb64-113f-4751-e870-44708e7a0434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "# Convert y_data to one-hot encoding\n",
        "y_train_oh = pd.get_dummies(y_train)\n",
        "y_test_oh = pd.get_dummies(y_test)\n",
        "\n",
        "\n",
        "display(y_train_oh.head(2))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0  1  2  3  4\n",
              "0  0  0  0  1  0\n",
              "1  0  0  1  0  0"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jij1UfpatgYa",
        "colab_type": "code",
        "outputId": "39aac4d0-cc32-48ef-b6df-b778607ca08a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "!pip3 install emoji\n",
        "import emoji\n",
        "\n",
        "# Prepare and show emoji dictionary\n",
        "emoji_dict = {  0: emoji.emojize(\":heart:\", use_aliases=True),   \n",
        "                1: emoji.emojize(\":baseball:\", use_aliases=True),\n",
        "                2: emoji.emojize(\":smile:\", use_aliases=True),\n",
        "                3: emoji.emojize(\":disappointed:\", use_aliases=True),\n",
        "                4: emoji.emojize(\":fork_and_knife:\", use_aliases=True)}\n",
        "\n",
        "print(\"Emoji tagrets:\")\n",
        "for i in range(5):\n",
        "    print(\"\\t\" + emoji_dict[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.5.4)\n",
            "Emoji tagrets:\n",
            "\t‚ù§\n",
            "\t‚öæ\n",
            "\tüòÑ\n",
            "\tüòû\n",
            "\tüç¥\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFAq5ZyiNn3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Loading w2v dictanionary\n",
        "# https://www.kaggle.com/ankitswarnkar/word-embedding-using-glove-vector\n",
        "def get_w2v_dict(file_name=\"glove.twitter.27B.50d.txt\"):\n",
        "    with open(PROJECT_PATH + file_name,'r') as f:\n",
        "        word2vector = {}\n",
        "        for line in f:\n",
        "            line_ = line.strip() \n",
        "            words_Vec = line_.split()\n",
        "            word2vector[words_Vec[0]] = np.array(words_Vec[1:],dtype=float)\n",
        "    return word2vector\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhspVCmjPa3u",
        "colab_type": "text"
      },
      "source": [
        "## Code for task #1: \n",
        "Convert sentences to average word embeddings (after converting every \n",
        "sentence to lower-case, and splitting the sentence into a list of words).\n",
        "Alternatively, convert sentences to min and max word embeddings (by\n",
        "taking element-wise min or max over the word embedding vectors)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2DJVyqTqQ4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoding data to vector reprezentation\n",
        "def get_encoded_data(data=None, emb_type=\"mean\", v2w_dict=None):\n",
        "    X_train_encoded = []\n",
        "    for sentence in data: \n",
        "        sentence_words = (sentence.lower()).split()\n",
        "\n",
        "        encoded_words = []\n",
        "        for word in sentence_words: \n",
        "            try:\n",
        "                encoded_words.append(v2w_dict[word])\n",
        "            except KeyError:\n",
        "                pass\n",
        "                print(\"Word Not found: \" + word  )\n",
        "        \n",
        "        emb_mean = np.mean(encoded_words, axis=0)\n",
        "        emb_min = np.min(encoded_words, axis=0)\n",
        "        emb_max = np.max(encoded_words, axis=0)\n",
        "        \n",
        "        if emb_type == \"mean\":\n",
        "            X_train_encoded.append(emb_mean)\n",
        "        elif emb_type == \"min\":\n",
        "            X_train_encoded.append(emb_min)\n",
        "        elif emb_type == \"max\":\n",
        "            X_train_encoded.append(emb_max)\n",
        "        elif emb_type == \"combined\":\n",
        "            tmp = np.concatenate((emb_mean, emb_min), axis=0)\n",
        "            tmp2 = np.concatenate((tmp, emb_max), axis=0)\n",
        "            X_train_encoded.append(tmp2)\n",
        "\n",
        "    shape = X_train_encoded[0].shape\n",
        "    X_train_df = pd.DataFrame(data=X_train_encoded) \n",
        "    return X_train_df, shape\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smJCyhpHwJV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Single layer softmax model \n",
        "def get_compiled_model(input_shape):\n",
        "\n",
        "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    flattened_inputs = tf.keras.layers.Flatten()(inputs)\n",
        "    scores = tf.keras.layers.Dense(5, activation='softmax',\n",
        "                             kernel_initializer=initializer)(flattened_inputs)\n",
        "\n",
        "    # Instantiate the model given inputs and outputs.\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=scores)\n",
        "\n",
        "    sgd = tf.keras.optimizers.SGD(lr=0.01, decay=0.000225, momentum=0.5)\n",
        "\n",
        "    model.compile(optimizer=sgd,\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbzUnHF7_739",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Turn off errors for nicer otuput\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "\n",
        "# Confusion matrix modified from: \n",
        "# https://github.com/omerbsezer/LSTM_RNN_Tutorials_with_Demo/blob/master/SentimentAnalysisProject/emo_utils.py\n",
        "def plot_confusion_matrix(y_actu, y_pred):\n",
        "    cmap=plt.cm.gray_r\n",
        "    df_confusion = pd.crosstab(y_actu, y_pred.reshape(y_pred.shape[0],), rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
        "    \n",
        "    df_conf_norm = df_confusion / df_confusion.sum(axis=1)\n",
        "    \n",
        "    plt.matshow(df_confusion, cmap=cmap) \n",
        "\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(df_confusion.columns))\n",
        "\n",
        "    labels = [emoji_dict[x] for x in range(5)]\n",
        "    labels.append(\"All\")\n",
        "\n",
        "    plt.xticks(tick_marks, labels)\n",
        "    plt.yticks(tick_marks, labels)\n",
        "\n",
        "    plt.ylabel(df_confusion.index.name)\n",
        "    plt.xlabel(df_confusion.columns.name)\n",
        "\n",
        "\n",
        "\n",
        "def train_model( w2v_dict=None, emb_type=\"mean\", dict_dataset=\"Glove Twitter\", display_details=False):\n",
        "    \n",
        "    X_train_encoded, input_shape = get_encoded_data(X_train,emb_type=emb_type, v2w_dict=w2v_dict)\n",
        "    X_test_encoded, _ = get_encoded_data(X_test, emb_type=emb_type, v2w_dict=w2v_dict)\n",
        "\n",
        "    model = get_compiled_model(input_shape)\n",
        "\n",
        "    \n",
        "    model.fit(X_train_encoded,\n",
        "              y_train_oh,\n",
        "              epochs=X_train_encoded.shape[0],\n",
        "              verbose=False)\n",
        "    \n",
        "    \n",
        "    loss, acc = model.evaluate(X_test_encoded,\n",
        "                           y_test_oh,\n",
        "                           steps=X_test_encoded.shape[0], \n",
        "                           verbose=False)\n",
        "\n",
        "    print(f\"Model using {dict_dataset} dict with {emb_type} embedding encoding to vector size of {input_shape} has accuracy of {acc}\")\n",
        "\n",
        "    if display_details == False:\n",
        "        return \n",
        "\n",
        "    test_samples_len = X_test_encoded.shape[0]\n",
        "    y_pred = np.zeros(test_samples_len)\n",
        "    predictions = model.predict(X_test_encoded)\n",
        "    \n",
        "\n",
        "\n",
        "    for i in range(test_samples_len):\n",
        "        y_pred[i] = np.argmax(predictions[i])\n",
        "\n",
        "    plot_confusion_matrix(y_test, y_pred, title='Confusion matrix', cmap=plt.cm.gray_r)\n",
        "    correct = []\n",
        "    wrong = []\n",
        "  \n",
        "    \n",
        "    for i in range(test_samples_len):\n",
        "        if y_pred[i] == y_test[i]:\n",
        "            correct.append(\"Correct prediction for: {0:<50} Expected/Predicted: {1:<1}/{2:<1}\".format(X_test[i], emoji_dict[y_test[i]], emoji_dict[y_pred[i]] ))\n",
        "        else:\n",
        "            wrong.append(\"Wrong prediction for: {0:<50} Expected/Predicted: {1:<1}/{2:<1}\".format(X_test[i], emoji_dict[y_test[i]], emoji_dict[y_pred[i]] ))\n",
        "\n",
        "    print(\"\\n\\nCorrect predictions:\")\n",
        "    for i in range(10):\n",
        "        print(\"{0:>3}) \".format(str(i+1)) +  correct[i])\n",
        "    \n",
        "    print(\"\\n\\nWrong predictions:\")\n",
        "    for i in range(10):\n",
        "        print(\"{0:>3}) \".format(str(i+1)) + wrong[i])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-_hm203JHhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load vocabs: \n",
        "\n",
        "# twitter dict \n",
        "w2v_twit_50 = get_w2v_dict(\"glove.twitter.27B.50d.txt\")\n",
        "w2v_twit_100 = get_w2v_dict(\"glove.twitter.27B.100d.txt\")\n",
        "w2v_twit_200 = get_w2v_dict(\"glove.twitter.27B.200d.txt\")\n",
        "\n",
        "# wiki dict\n",
        "w2v_wiki_50 = get_w2v_dict(\"glove.6B.50d.txt\")\n",
        "w2v_wiki_100 = get_w2v_dict(\"glove.6B.100d.txt\")\n",
        "w2v_wiki_200 = get_w2v_dict(\"glove.6B.200d.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-N8fy5urDuzq"
      },
      "source": [
        "## Answers and output for task #2\n",
        "\n",
        "Use GloVe embeddings of size 50 (trained on Twitter) to train and evaluate the classification model described above. \n",
        "\n",
        "**Show the confusion matrix.**\n",
        "> In the cell output\n",
        "\n",
        "**What is the accuracy of the classifier on the test dataset?** \n",
        "> In the cell output\n",
        "\n",
        "**Show 10 examples that are correctly classified by the model trained, and 10 examples that are misclassified by the model.**\n",
        "> In the cell output\n",
        "\n",
        "\n",
        "**Are the results surprising at all?**\n",
        "\n",
        "_For me it was a bit suprising, how good the model actually got, given the fact it was trained on small dataset and sentences from test dataset had a lot of unseen words._\n",
        "\n",
        "_On the otherhand it demonstrates, how well are the words encoded in embedding vector._\n",
        "\n",
        "\n",
        "**Can you explain the misclassifications?**\n",
        " \n",
        "_In some cases the misclassification I think can be explained as only missing true positive response such as with some close words from train dataset wich are in similar vector space. These sentencs would be:_\n",
        "> I miss you so much                                 Expected/Predicted: ‚ù§/üòÑ\n",
        "\n",
        "> My grandmother is the love of my life              Expected/Predicted: ‚ù§/üòÑ\n",
        " \n",
        "> she got me a nice present                          Expected/Predicted: üòÑ/‚ù§\n",
        "\n",
        "> where is the food                                  Expected/Predicted: üç¥/üòÑ\n",
        "\n",
        "> I love taking breaks                               Expected/Predicted: ‚ù§/üòû\n",
        "\n",
        "_In some cases it is just from low amount of data:_\n",
        "> you brighten my day                                Expected/Predicted: üòÑ/üòû\n",
        "\n",
        "> she is a bully                                     Expected/Predicted: üòû/üòÑ\n",
        "\n",
        "> enjoy your game                                    Expected/Predicted: ‚öæ/üòû\n",
        "\n",
        "_And here probably average \"work is\" embedding is close and to learned wight for smiley predictions_\n",
        "\n",
        "> work is hard                                       Expected/Predicted: üòû/üòÑ\n",
        "\n",
        "> work is horrible                                   Expected/Predicted: üòû/üòÑ\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBcOGkHHHIgO",
        "colab_type": "code",
        "outputId": "2dd8fbce-919f-4c02-e349-6ee21c8b45b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        }
      },
      "source": [
        "train_model( w2v_dict=w2v_twit_50, emb_type=\"mean\", dict_dataset=\"Glove Twitter\", display_details=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model using Glove Twitter dict with mean embedding encoding to vector size of (50,) has accuracy of 0.6071428656578064\n",
            "\n",
            "\n",
            "Correct predictions:\n",
            "  1) Correct prediction for: I want to eat                                      Expected/Predicted: üç¥/üç¥\n",
            "  2) Correct prediction for: he did not answer                                  Expected/Predicted: üòû/üòû\n",
            "  3) Correct prediction for: he got a very nice raise                           Expected/Predicted: üòÑ/üòÑ\n",
            "  4) Correct prediction for: ha ha ha it was so funny                           Expected/Predicted: üòÑ/üòÑ\n",
            "  5) Correct prediction for: he is a good friend                                Expected/Predicted: üòÑ/üòÑ\n",
            "  6) Correct prediction for: I am upset                                         Expected/Predicted: üòû/üòû\n",
            "  7) Correct prediction for: We had such a lovely dinner tonight                Expected/Predicted: üòÑ/üòÑ\n",
            "  8) Correct prediction for: Stop making this joke ha ha ha                     Expected/Predicted: üòÑ/üòÑ\n",
            "  9) Correct prediction for: where is the ball                                  Expected/Predicted: ‚öæ/‚öæ\n",
            " 10) Correct prediction for: This girl is messing with me                       Expected/Predicted: üòû/üòû\n",
            "\n",
            "\n",
            "Wrong predictions:\n",
            "  1) Wrong prediction for: she got me a nice present                          Expected/Predicted: üòÑ/‚ù§\n",
            "  2) Wrong prediction for: where is the food                                  Expected/Predicted: üç¥/üòÑ\n",
            "  3) Wrong prediction for: work is hard                                       Expected/Predicted: üòû/üòÑ\n",
            "  4) Wrong prediction for: work is horrible                                   Expected/Predicted: üòû/üòÑ\n",
            "  5) Wrong prediction for: I love taking breaks                               Expected/Predicted: ‚ù§/üòû\n",
            "  6) Wrong prediction for: you brighten my day                                Expected/Predicted: üòÑ/üòû\n",
            "  7) Wrong prediction for: she is a bully                                     Expected/Predicted: üòû/üòÑ\n",
            "  8) Wrong prediction for: My grandmother is the love of my life              Expected/Predicted: ‚ù§/üòÑ\n",
            "  9) Wrong prediction for: enjoy your game                                    Expected/Predicted: ‚öæ/üòû\n",
            " 10) Wrong prediction for: I miss you so much                                 Expected/Predicted: ‚ù§/üòÑ\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAAD3CAYAAADormr9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzIFhZskcGGaWzIFhZskcGGaW7P8BCfoAKwWqWdkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 288x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBjrWw_oJ_db",
        "colab_type": "text"
      },
      "source": [
        "## Anwers and output for task #3\n",
        "Compare the results (accuracy) of the models obtained with avg/min/max sentence representations, respectively. \n",
        "\n",
        "**Which representation gives the best results?**\n",
        "> Results in a cell outputs\n",
        "\n",
        "_Depends on a run, all are pretty close to each other in  terms of accuracy on test data, in this case best score achieved by a min sentece embedding (0.64)._\n",
        "\n",
        "**Do the results improve if you use avg/min/max together (in this case, the sentence vectors will be 3x50)?**\n",
        "\n",
        "_Yes quite a lot, more information improvess accuracy to 0.76._ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndgkQgHvs9vS",
        "colab_type": "code",
        "outputId": "3510f55c-7014-442c-8524-e3bfd458cdc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "\n",
        "train_model( w2v_dict=w2v_twit_50, emb_type=\"mean\", dict_dataset=\"Glove Twitter\", display_details=False)\n",
        "train_model( w2v_dict=w2v_twit_50, emb_type=\"min\", dict_dataset=\"Glove Twitter\", display_details=False)\n",
        "train_model( w2v_dict=w2v_twit_50, emb_type=\"max\", dict_dataset=\"Glove Twitter\", display_details=False)\n",
        "train_model( w2v_dict=w2v_twit_50, emb_type=\"combined\", dict_dataset=\"Glove Twitter\", display_details=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model using Glove Twitter dict with mean embedding encoding to vector size of (50,) has accuracy of 0.6071428656578064\n",
            "Model using Glove Twitter dict with min embedding encoding to vector size of (50,) has accuracy of 0.6785714030265808\n",
            "Model using Glove Twitter dict with max embedding encoding to vector size of (50,) has accuracy of 0.6428571343421936\n",
            "Model using Glove Twitter dict with combined embedding encoding to vector size of (150,) has accuracy of 0.7678571343421936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CevrKDRMXB8",
        "colab_type": "text"
      },
      "source": [
        "## Answers and output for task #5\n",
        "\n",
        "Train models using the 50d, 100d, and 200d Glove embeddings using both\n",
        "the set trained on Wikipedia and the set trained on Twitter. \n",
        "\n",
        "**For each set, what embedding dimension gives better results?** \n",
        "_Here the bigger embedding size is the better are the results for both of the wiki/twitter datasets_ \n",
        "\n",
        "**Between the two sets (Wikipedia versus Twitter), which one performs better?**\n",
        "\n",
        "_In average wikidia set has better results_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYC3G5lZ4Yq-",
        "colab_type": "code",
        "outputId": "ebbdc370-0f4f-41a5-e6f6-ef356e93497f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "train_model( w2v_dict=w2v_twit_50, emb_type=\"mean\", dict_dataset=\"Glove Twitter\", display_details=False)\n",
        "train_model( w2v_dict=w2v_twit_100, emb_type=\"mean\", dict_dataset=\"Glove Twitter\", display_details=False)\n",
        "train_model( w2v_dict=w2v_twit_200, emb_type=\"mean\", dict_dataset=\"Glove Twitter\", display_details=False)\n",
        "\n",
        "train_model( w2v_dict=w2v_wiki_50, emb_type=\"mean\", dict_dataset=\"Glove Wiki\", display_details=False)\n",
        "train_model( w2v_dict=w2v_wiki_100, emb_type=\"mean\", dict_dataset=\"Glove Wiki\", display_details=False)\n",
        "train_model( w2v_dict=w2v_wiki_200, emb_type=\"mean\", dict_dataset=\"Glove Wiki\", display_details=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model using Glove Twitter dict with mean embedding encoding to vector size of (50,) has accuracy of 0.6428571343421936\n",
            "Model using Glove Twitter dict with mean embedding encoding to vector size of (100,) has accuracy of 0.6964285969734192\n",
            "Model using Glove Twitter dict with mean embedding encoding to vector size of (200,) has accuracy of 0.75\n",
            "Model using Glove Wiki dict with mean embedding encoding to vector size of (50,) has accuracy of 0.6785714030265808\n",
            "Model using Glove Wiki dict with mean embedding encoding to vector size of (100,) has accuracy of 0.6607142686843872\n",
            "Model using Glove Wiki dict with mean embedding encoding to vector size of (200,) has accuracy of 0.7857142686843872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4uDsbVuroq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Making RAM space \n",
        "del w2v_twit_50\n",
        "del w2v_twit_100\n",
        "del w2v_twit_200\n",
        "del w2v_wiki_50\n",
        "del w2v_wiki_100\n",
        "del w2v_wiki_200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh-RLiJBaDSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install gensim\n",
        "from gensim import models\n",
        "\n",
        "model = models.KeyedVectors.load_word2vec_format(PROJECT_PATH + 'GoogleNews-vectors-negative300.bin.gz', binary=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi2EcRu-NjeK",
        "colab_type": "text"
      },
      "source": [
        "## Answers and output for task #4\n",
        "\n",
        "Use 300d Word2vec and 300d Glove embeddings (trained on Wikipedia) to train the model with the average word embedding representation for sentences. \n",
        "\n",
        "**Between 300dWord2vec embeddings and 300d GloVe embeddings, which set performs better?**\n",
        "\n",
        "_The glove ouperformed the 300dWord2vec, but both are does not perform better than the 200d._\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngi1WM9bLBU3",
        "colab_type": "code",
        "outputId": "c0226c3e-365f-429b-a6f0-a2361d274dd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "w2v_gn_300 = model.wv\n",
        "\n",
        "train_model( w2v_dict=w2v_gn_300, emb_type=\"mean\", dict_dataset=\"Word2Vec Google News\", display_details=False)\n",
        "\n",
        "# Free memory again\n",
        "del model\n",
        "del w2v_gn_300\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model using Word2Vec Google News dict with mean embedding encoding to vector size of (300,) has accuracy of 0.5892857313156128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LC2xHzJmTRd",
        "colab_type": "code",
        "outputId": "62384f72-e893-4614-e372-5fbc1c36641d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "w2v_gn_300 = get_w2v_dict(\"glove.6B.300d.txt\")\n",
        "train_model( w2v_dict=w2v_gn_300, emb_type=\"mean\", dict_dataset=\"Glove Wiki\", display_details=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model using Glove Wiki dict with mean embedding encoding to vector size of (300,) has accuracy of 0.7321428656578064\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}